# -*- coding: utf-8 -*-
"""CPSC-585-Project-02-Group 7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hm1huK8ere_Cb1wkObYjFDrDqFa2EJNO

```
# This is formatted as code
```

Team: Jarrod Leong : 885142281,
      Jeet Hirakani : 885189175, 
      Arjit Saxena: 885188839, 
      Dhyey Desai : 885451609, 
      Ayush Bhardwaj : 885866178, 
      Janvier Uwase: 885942086

#**Steps to run the first model/best model**
* To run the first model, you need to create a folder in your Google Drive named "**CPSC585_Project3**".

* Inside that folder you need to create 2 more folders titled "**data**" and "**weights**".

* Upload the RateMyProfessor's Sample data into the "**data**" folder.

# **DataSet and Data PreProcessing**

## **Data Cleanup**
Below is code that cleanups the data and extracts only relevant columns in the dataset. It removes the rows with missing comments and ratings (only a few rows were removed) and runs a spell check on the words because there were many spelling errors found in the comments.  It also only extracts relevant rows: 
1. student_star
2. student_difficult
3. comments
*Note that this process will take a while since the spellchecker is not fast. As a result, it was run locally and the data was reuploaded to the drive, hence the failed/non existent output.*
"""

from google.colab import drive
drive.mount('/content/drive/')
drive_path = '/content/drive/MyDrive/CPSC585_Project3'

!pip install pyspellchecker

import csv
from spellchecker import SpellChecker
import re

def read_csv(path: str):
    '''
    reads csv and returns data as list
    '''
    rows = []
    with open(path, 'r') as file:
        reader = csv.DictReader(file)
        for row in reader:
            if row['comments'] != '':
                rows.append((row['comments'], float(row['student_star']), float(row['student_difficult'])))

    print('extracted data from: ', path)

    return rows

def write_new_data(path: str, data: list):
    '''
    writes data to new csv
    '''
    print('running spellchecker on data and saving to new file: ', path)
    with open(path, 'w', newline='') as file:
        fieldnames = ['student_star', 'student_difficult', 'comments']
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        writer.writeheader()
        for row in data:
            writer.writerow({'comments': spellcheck(row[0]), 'student_star': row[1], 'student_difficult': row[2]})



def spellcheck(s: str):
    sp = SpellChecker()
    words = s.split()
    corrected = []

    for w in words:
        w = w.strip()
        punct = w[-1] if not w[-1].isalnum() else ''
        corr = sp.correction(w)
        corrected.append(corr+punct if corr is not None else w)

    return ' '.join(corrected)


data = read_csv(drive_path + '/data/RateMyProfessor_Sample data.csv')
write_new_data(drive_path + '/data/RMP_clean2.csv', data)

"""## **DataSet Creation**
For representing the data, we created a custom DataSet class called RMPDataSet. the dataset takes in the csv path and loads the data from the csv. Next it loads the vocab from the GloVe embeddings. It then converts the sentences into tokens and then converts the tokens into integers via the vocab vectors of the GloVe embeddings. We add \<unk> tokens for words not in the vocab list and \<pad> for padding tokens so that training input data is the same size.
"""

from torch.utils.data import Dataset
import torch
import numpy as np
import csv
from torchtext.vocab import GloVe, vocab

class RMPDataSet(Dataset):
    def __init__(self, csv_path: str):

        self.x, self.y = self.load_data(csv_path)
        #self.y = self.load_data(targetpath)  

        assert len(self.x) == len(self.y)  


    def load_data(self, path:str):
        '''
        loads data from given path and returns data as tensor
        '''
        vectors = GloVe(cache=drive_path + '/vocab/.vector_cache')
        words = vocab(vectors.stoi)
        words.insert_token('<unk>', 0)
        words.set_default_index(0)
        words.insert_token('<pad>', 1)
        

        inputs = []
        outputs = []
        with open(path, 'r') as file:
            reader = csv.reader(file)
            next(reader)
            for row in reader:
                inputs.append(torch.tensor(words(row[2].split())))
                outputs.append([float(row[0]), float(row[1])])

        return torch.nn.utils.rnn.pad_sequence(inputs,batch_first=True, padding_value=1), torch.tensor(outputs)
            


    def __len__(self) -> int:
        return len(self.x)
    
    def __getitem__(self, index) -> tuple:
        return self.x[index], self.y[index]

# testing Dataset
data = RMPDataSet(drive_path+'/data/RMP_clean.csv')
print(data.x[0])
print(data.y[0])
print(len(data.x[0]))
print(data.x.shape())
print(data.y.shape())

"""# **Model Creation**
For the best performing model, we used a bidirectional GRU. Our model used 3 layers with a hidden size of 128. We used a dropout of 0.2 to prevent overfitting. The first layer is the embedding layer which creates embeddings with the GloVe embeddings 840B. The next layer is the GRU layer and the final layers are the fully connect dense layers that outputs out 2 values in the final layer, one for the star rating and one for the difficulty rating.
"""

import torch.nn as nn
import torch.nn.functional as F
import torch
from torchtext.vocab import GloVe, vocab

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class RMPModelBi(nn.Module):
    def __init__(self, num_layers=5, hidden_size=256):
        super(RMPModelBi, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        glove_vectors = GloVe(cache=drive_path + '/vocab/.vector_cache')
        self.oov = '<unk>'
        self.pad = '<pad>'
        self.vocab_words = vocab(glove_vectors.stoi)
        self.vocab_words.insert_token(self.oov, 0)
        self.vocab_words.insert_token(self.pad, 1)
        self.vocab_words.set_default_index(0)

        self.embedding = nn.Embedding.from_pretrained(glove_vectors.vectors, freeze=True)
        self.gru = nn.GRU(input_size=300, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0.2, bidirectional=True)
        
        self.activation = F.relu
        self.fc1 = nn.Linear(self.hidden_size*2, self.hidden_size)
        self.fc2 = nn.Linear(self.hidden_size, 2)


    def lookup_toks(self, tokens: list):
        return self.vocab_words.lookup_tokens(tokens)

    def forward(self, x):
        x = self.embedding(x)
        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(DEVICE)
        x, _ = self.gru(x, h0)

        x = x[:, -1, :]
        
        x = self.activation(self.fc1(x))
        x = self.fc2(x)

        return x

"""# **Training**

## **Hyperparameters and Configurations**

below is where we set some parameters and hyperparameters used in our model and training.
- We use a learning rate of 0.0001
- We set a seed of 419 to make sure we are traing consistently.
- MSE is chosen for our loss function since this is a regression problem.
- We use the AdamW optimizer
- a batch size of 32 is chosen
- we set L2 term to 4e-3
- Lastly we add a learning rate scheduler to decrease the learning rate when it starts to overfit, this was tested to be around the 20th epoch.
"""

from random import seed
from torch.serialization import save
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
import torch
import sys
import time
from argparse import ArgumentParser


SAVEPATH = drive_path + '/weights/weights1.pth'
MODELTYPE = RMPModelBi
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('USING DEVICE: ', DEVICE)
DATASET =  RMPDataSet

# HYPERPARAMETERS
NUM_LAYERS = 3
HIDDEN_SIZE = 128

SEED = 419
LEARNING_RATE = .0001
L2 = 4e-3
CRITERION = torch.nn.MSELoss
OPTIMIZER = torch.optim.AdamW   # just the type. will be initialized below
#OPTIMIZER = torch.optim.RMSprop
BATCH_SIZE = 32

# Scheduler settings
USE_SCHEDULER = True
SCHEDULER_STEP = 20
GAMMA = 0.1

"""## **Extra Functions**
Below are extra functions defined that are used in our training.
- load_data()
  - loads the data into 3 splits: training, validation, and test
- save_params()
  - writes the params used to a file
- save_test()
  - saves the test results of the given dataset to a file
- evaluate()
  - evaluates the dataset given through the model
  
"""

def load_data() -> tuple:
    '''
    loads data and returns tuple of size 3 with training, validation, and test data
    '''

    dataset = DATASET(drive_path + '/data/RMP_clean.csv')
    generator = torch.Generator().manual_seed(42)
    train, val, test = random_split(dataset, [.8, .1, .1], generator=generator)

    train_dl = DataLoader(train, batch_size = BATCH_SIZE, shuffle = True, pin_memory=True)
    val_dl = DataLoader(val, batch_size = 1, shuffle = False)
    test_dl = DataLoader(test, batch_size = 1, shuffle = False)    
    
    print('Successfully loaded dataset.')

    return train_dl, val_dl, test_dl


def saveparams(path, seed, lr, optimizer, criterion):
    with open(path, 'w') as file:
        file.write(f'SEED:\t{seed}\n')
        file.write(f'LEARNING RATE:\t{lr}\n')
        file.write(f'LOSS FUNCTION:\t{criterion}\n')
        file.write(f'OPTIMIZER:\t{optimizer}\n')


def save_test(testresults, path=drive_path + '/data/testresults.txt') -> float:
    '''
    saves test results to a file
    '''
    criterion = CRITERION()

    sorted_results = {'target': [], 'predicted':[]}
    expected = [(i, ex) for i, ex in enumerate(testresults['loss'])]
    expected.sort(key = lambda x: x[1].item())
    sorted_results['loss'] = [i[1] for i in expected]
    sorted_results['predicted'] = [testresults['predicted digit'][i[0]] for i in expected]
    sorted_results['input'] = [testresults['input'][i[0]] for i in expected]
    sorted_results['target'] = [testresults['target digit'][i[0]] for i in expected]
    sorted_results['star_loss'] = [criterion(sorted_results['target'][i][0], sorted_results['predicted'][i][0]) for i in range(len(sorted_results['target']))]
    sorted_results['diff_loss'] = [criterion(sorted_results['target'][i][1], sorted_results['predicted'][i][1]) for i in range(len(sorted_results['target']))]


    # saves sorted results to a file
    with open(path, 'w') as file:
        #totalerror = sum([sum(abs(testresults['predicted'][i] - testresults['target'][i])).item() for i in range(len(testresults['predicted']))])
        #totalerror = sum([criterion(testresults['predicted'][i], testresults['target'][i]).item() for i in range(len(testresults['predicted']))])
        totalerror = sum(sorted_results['loss'])
        avgitemloss =  totalerror / len(testresults['predicted'])
        avg_star = sum(sorted_results['star_loss'])/len(sorted_results['star_loss'])
        avg_diff = sum(sorted_results['diff_loss'])/len(sorted_results['diff_loss'])

        file.write('RESULTS\n')
        file.write('-'*60)
        file.write('\nTOTAL ERROR: ' + str(round(totalerror.item(), ndigits=3)) + '\t AVERAGE ERROR: ' + str(round(avgitemloss.item(), ndigits=3)) + '\n')
        file.write('AVERAGE STAR LOSS: ' + str(round(avg_star.item(), ndigits=3)))
        file.write('\tAVERAGE DIFFICULTY LOSS: ' + str(round(avg_diff.item(), ndigits=3)) + '\n')
        file.write('-' * 60 + '\n')
        for i in range(len(sorted_results['predicted'])):
            file.write(
                    '[expected]:  ' + str([round(x.item(), 1) for x in sorted_results['target'][i]]) + 
                    '  \t[predicted]:  ' + str([round(x.item(), 1) for x in sorted_results['predicted'][i]]) + 
                    '  \t[star_loss]:   ' + str(round(sorted_results['star_loss'][i].item(), ndigits=2)) +
                    '  \t[diff_loss]:   ' + str(round(sorted_results['diff_loss'][i].item(), ndigits=2)) +
                    '  \t[total_loss]:  ' + str(round(sorted_results['loss'][i].item(), ndigits=2)) +
                    '  \t[input]: ' + sorted_results['input'][i] +'\n'
                    )

    return avgitemloss.item(), totalerror.item(), avg_star.item(), avg_diff.item()




def evaluate(model, data, path=drive_path + '/data/testresults.txt') -> float:
    '''
    Runs validation on test data set. Saves results to a file. returns avg item loss of test set.
    '''
    model.eval()
    model.to(DEVICE)
    criterion = CRITERION()
    
    testresults = {'input': [], 'target': [], 'predicted': [], 'target digit': [], 'predicted digit': [], 'loss': []}

    #iterates over data set
    with torch.no_grad():
        for  i,(inputs, targets) in enumerate(data):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)

            yhat = model(inputs).squeeze(-1)

            testresults['input'].append(' '.join(model.lookup_toks(list(inputs[0]))).split('<pad>')[0])
            testresults['target'].append(targets[0])
            testresults['predicted'].append(yhat[0])
            testresults['target digit'].append(targets[0])
            testresults['predicted digit'].append(yhat[0])
            testresults['loss'].append(criterion(targets[0], yhat[0]))

        
        avgitemloss, totalloss, avg_star, avg_diff = save_test(testresults, path = path)
        model.train()
        return avgitemloss, totalloss, avg_star, avg_diff

"""## Training Loop

## **Setup training function**
The training loop is defined below. It runs through the entire epoch and then checks if the validation loss has improved. If it has, it saves the weights to a file. A SummaryWriter for tensorboard is also included to visualize how the loss changes over the course of the training.
"""

def train(
        parser: ArgumentParser, 
        savepath:str=SAVEPATH, 
        modeltype=MODELTYPE,
        seed:int=SEED, 
        lr:float=LEARNING_RATE, 
        criterion=CRITERION, 
        optimizer=OPTIMIZER, 
        load_data=load_data
        ):
    '''
    runs training loop for model with given parser params and hyperparameters entered.

    Args:
        parser:     CLI parser args
        savepath (str, optional):        path to save/retreive weights
        modeltype (nn.Module, optional): model to use 
        seed (int, optional):            sets seed to this value
        lr (float, optional):            learning rate
        criterion (optional):            loss function to be used
        optimizer (optional):            optimizer to be used
        load_data (func, optional):      function to load data. Must return 3-item-tuple (train, val, test) DataLoaders
    '''
    if not parser.test:
        writer = SummaryWriter(log_dir=drive_path + '/runs')
        layout = {
            'Summary': {
                'loss': ['Multiline', ['loss/train', 'loss/validation', 'loss/validation_star', 'loss/validation_difficulty']],
                # 'accuracy': ['Multiline', ['accuracy/train', 'accuracy/validation']]
            }
        }

        writer.add_custom_scalars(layout)

    torch.manual_seed(seed)

    train_dl, val_dl, test_dl = load_data()
    model = modeltype(num_layers=NUM_LAYERS, hidden_size=HIDDEN_SIZE)
    model = model.to(DEVICE)

    #default loads weights otherwise does not try to load weights.
    if not parser.new or parser.test:
        print('loading weights: ', savepath + '...')
        model.load_state_dict(torch.load(savepath))
        print('successfully loaded weights')

    if parser.test:
        print('performing test...')
        use_dl = test_dl
        model.eval()
        avgloss, totalloss, avg_star, avg_diff = evaluate(model, use_dl)
        print('finished test. find results in /data/testresults.txt')
        print(
            'total loss: ', round(totalloss, ndigits=3), 
            '  avg loss: ', round(avgloss, ndigits=3), 
            ' avg star loss: ', round(avg_star, ndigits=3), 
            ' avg diff loss: ', round(avg_diff, ndigits=3)
            )
        return
    else:
        use_dl = train_dl
        model.train()

    criterion = criterion()
    optimizer = optimizer(model.parameters(), lr = lr, weight_decay = L2)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = SCHEDULER_STEP, gamma = GAMMA)

    best_loss = -1  # validation set

    #sets epoch count
    epoch_count = parser.epochs
    if parser.test:
        epoch_count = 1


    for epoch in range(epoch_count):
        starttime = time.time()
        total_loss = []
        batchcount = []
        target_loss = 0 #how many it got incorrect
        avg_loss = 0    #used to find avg loss for each individual item
        
        testresults = {'expected': [], 'predicted': []}

        #iterates over data set
        for  i,(inputs, targets) in enumerate(use_dl):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)

            yhat = model(inputs).squeeze(-1)

            loss = criterion(yhat, targets)

            #adds to testresults if testing
            if parser.test:
                testresults['expected'].append(targets[0].item())
                testresults['predicted'].append(yhat[0].item())


            #gets data to graph and computate losses
            total_loss.append(loss.item())
            batchcount.append(i)            

            if parser.test:
                torch.cuda.empty_cache()

            #backpropogation
            if not parser.test:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        #runs validation set
        valloss, valacc, avg_star, avg_diff = evaluate(model, val_dl)

        #saves model if it improves based on validation
        if not parser.test and (best_loss == -1 or  best_loss > valloss):
            print('saving model...')
            torch.save(model.state_dict(), savepath)
            parampath = '.'.join(savepath.split('.')[:-1]) + '.params'
            saveparams(parampath, seed, lr, optimizer, criterion)
            best_loss = valloss

        if not parser.test:
            totaltime = round(time.time() - starttime)
            print('[EPOCH:', epoch, 'TIME: {:d}hr {:d}m {:d}s]:'.format(totaltime//60//60, totaltime//60%60, totaltime%60),
            '  total loss: ', round(sum(total_loss), ndigits=5), 
            '  avg target loss: ', round(sum(total_loss)/len(train_dl), ndigits=5),
            '  avg val loss: ', round(valloss, ndigits=5),
            )

            writer.add_scalar("loss/train", sum(total_loss)/len(train_dl), epoch)
            writer.add_scalar("loss/validation", valloss, epoch)
            writer.add_scalar("loss/validation_star", avg_star, epoch)
            writer.add_scalar("loss/validation_difficulty", avg_diff, epoch)
            writer.flush()

            if USE_SCHEDULER:
                scheduler.step()
        
        else:
            avgloss, totalloss = save_test(testresults)
            print('finished test. find results in /data/testresults.txt')
            print('total loss: ', totalloss, '  avg loss: ', avgloss)

"""## **Run Training Loop**"""

parser = ArgumentParser()
parser.add_argument('-new', action='store_true')
parser.add_argument('-test', action='store_true')
parser.add_argument('-epochs', type=int, default=100)
# parser = parser.parse_args()

# adapted from local files, quick fix to run in Colab
parser.new = True
parser.epochs = 50
parser.test = False

train(parser)

"""# **Results and Testing**

Below is the tensorboard output of the training loop.
"""

# kill tensorboard
!kill 14230

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir  '/content/drive/MyDrive/CPSC585_Project3/runs/final'

"""## **Testing and Evaluation**
Tests and evaluations are shown below. The code snippet below runs the model on the the test data set and saves results to a file.
"""

parser = ArgumentParser()
parser.add_argument('-new', action='store_true')
parser.add_argument('-test', action='store_true')
parser.add_argument('-epochs', type=int, default=100)

parser.new = False
parser.test = True
train(parser, savepath=drive_path + '/weights/weights1_final.pth')

"""**The Above model gave us the best results for regression and the loss is minimum out of all tried models**

#**Test Results**
Test results are linked here in a separate Google Docs file as there are many pages, however a sample is shown below. The results are sorted by the total loss in Ascending order.

[Test Results](https://drive.google.com/file/d/1-JCx0M3gbiU_MfaGx_Hyk2ff_QhwX54m/view?usp=share_link)


```
RESULTS
------------------------------------------------------------
TOTAL ERROR: 2363.211	 AVERAGE ERROR: 1.182
AVERAGE STAR LOSS: 1.003	AVERAGE DIFFICULTY LOSS: 1.361
------------------------------------------------------------
[expected]:  [2.5, 3.0]  	[predicted]:  [2.5, 3.0]  	[star_loss]:   0.0  	[diff_loss]:   0.0  	[total_loss]:  0.0  	[input]: One word boring Bring your pillow to class 
[expected]:  [4.5, 3.0]  	[predicted]:  [4.5, 3.0]  	[star_loss]:   0.0  	[diff_loss]:   0.0  	[total_loss]:  0.0  	[input]: She is a great teacher and you will learn a lot from her Class work is very time consuming though so don't take an upper division class from her if you are not going to have a lot of time to put into the class Make sure to do the weekly assignments because they make a difference in your final grade Is always will to help you and go over things 
[expected]:  [2.5, 4.0]  	[predicted]:  [2.5, 3.9]  	[star_loss]:   0.0  	[diff_loss]:   0.0  	[total_loss]:  0.0  	[input]: The sources of material for her tests made studying kinda hard Some was from lecture some was from the book about half hard half easy to figure out what info she would test you on 
[expected]:  [2.0, 4.0]  	[predicted]:  [2.0, 4.0]  	[star_loss]:   0.0  	[diff_loss]:   0.0  	[total_loss]:  0.0  	[input]: Hard to take as an online class Did not return emails Very VERY slow at grading assignments Assignments are randomly placed throughout the semester Lets you know AFTER the deadline that the test period has been extended Not helpful when you ask her questions I would take another teacher if I had the opportunity to 
[expected]:  [4.5, 2.0]  	[predicted]:  [4.5, 1.9]  	[star_loss]:   0.0  	[diff_loss]:   0.01  	[total_loss]:  0.0  	[input]: Not sure how he got the negative ratings but the Prof was the exact opposite of what was stated on here Prof was very clear very good and the class was really interesting 
[expected]:  [1.5, 4.0]  	[predicted]:  [1.6, 4.1]  	[star_loss]:   0.0  	[diff_loss]:   0.0  	[total_loss]:  0.0  	[input]: She will not help you with any questions you have She will only tell you to reread the instructions and do nothing else She treats you like you are still in middle school and always talks down to you She tells you that you <unk> have items like laptops even though it is allowed in the cock school policy 
[expected]:  [1.5, 4.0]  	[predicted]:  [1.5, 4.1]  	[star_loss]:   0.0  	[diff_loss]:   0.01  	[total_loss]:  0.01  	[input]: Very theological Liberal narrow minded don't let him know if you disagree with his opinions he will explode Not a great teacher either EXPLOSIVE TEMPER 
[expected]:  [4.5, 2.0]  	[predicted]:  [4.6, 2.0]  	[star_loss]:   0.01  	[diff_loss]:   0.0  	[total_loss]:  0.01  	[input]: <unk> really helpful If <unk> anything you need help with <unk> willing to help you Students come first to him Great professor Class <unk> a blow off easy class like most think it is You have to work for the grade you get Has good humor and is pretty cool guys Make sure to come prepared to class at all <unk> saying 
[expected]:  [2.0, 4.0]  	[predicted]:  [1.9, 4.1]  	[star_loss]:   0.01  	[diff_loss]:   0.0  	[total_loss]:  0.01  	[input]: Where do i even <unk> <unk> terrible You have 3 tests and <unk> all hard considering the fact that she gives you 4 correct answers and you have to choose the most correct answer Make sure you understand APA front and back because she tears apart your paper for literally everything no point of going to class she reads off the up 
[expected]:  [4.0, 3.0]  	[predicted]:  [4.1, 3.0]  	[star_loss]:   0.01  	[diff_loss]:   0.0  	[total_loss]:  0.01  	[input]: Although she clearly cares a tonne for her students the teaching method can be a bit confusing If you want to learn <unk> help you Learn the midterm material perfectly because you <unk> have time to finish the exam i he exam in 2 <unk> I actually would recommend her as she is reasonable caring and available for meeting 

```

### **Test Results and Analysis**
We ultimately went with the GRU as it trained fast and got decent results. We used a bidirectional GRU to help maintain some future context of words that helped boost the accuracy a little. Evaluating the test results was interesting. We ended with an average error of 1.18 per item, which, for predicting subjective values is decent.

We also computed separate loss values for star ratings and difficulty ratings which showed some interesting results. We were able to predict the star rating with a loss of 1.003 but for the difficulty rating we had a loss of 1.361. We believe this can be explained because the comments by students often times held more sentiment toward how much they liked the teacher but less sentiment toward how hard the teacher is. This can explain why it was harder to predict the difficulty as less comments included things to help determine the difficulty of a teacher.

# **Other Models We Tried**

* This section includes the other methods we tried.

##Classification

We initially thought that it was a classification problem. So we trained classification models with Simple RNN and LSTM architecture.

###RNN Model 1

* For this model, we just used the keras embeddings
"""

import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical
from keras import Sequential
from keras.layers import Dense,SimpleRNN,Embedding,Flatten

dataset = pd.read_csv("RateMyProfessor_Sample data.csv")

dataset.head()

dataset['student_star']

dataset.isna().sum()

dataset.comments.fillna(" ",inplace=True)
dataset.student_star.fillna(0,inplace=True)
dataset.student_difficult.fillna(0,inplace=True)

"""Now determine difficulty and quality base on rating. If rating is below 4 then we will consider as bad quality or difficult subject. """

def sentiment(value):
  if (int(value) == 1 or int(value) == 2 or int(value) == 3):
    return 0 
  else:
    return 1

dataset.student_difficult  = dataset.student_difficult.apply(sentiment)
dataset.student_star = dataset.student_star.apply(sentiment)

dataset.student_difficult

tokenizer = Tokenizer()
tokenizer.fit_on_texts(dataset.comments)
len(tokenizer.word_index)

sequences = tokenizer.texts_to_sequences(dataset.comments)
sequences

padding_dataset = pad_sequences(sequences,padding='post', maxlen=100)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

y = le.fit_transform(dataset.student_difficult)
y

y[700]

dataset.student_difficult[700]

import numpy as np
y = to_categorical(np.asarray(y))
y

y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padding_dataset, y, test_size=0.2, random_state = 40)

X_train.shape

vocab_size = len(tokenizer.word_index) + 1
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=100,name="embedding"))
model.add(SimpleRNN(32,return_sequences=False, activation="relu"))
model.add(Dense(16, activation='relu'))
model.add(Dense(2, activation='softmax'))

model.summary()

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])
model.fit(X_train,y_train,epochs= 5,batch_size=32,verbose=True,validation_data=(X_test,y_test))

predictions = model.predict(X_test)
# print(predictions)
print(predictions[800])
print(y_test[800])

X_test.shape

X_test[7]

weights = model.get_layer('embedding').get_weights()[0]
weights[3]

weights[7]

"""###RNN Model 2

* This model has different RNN architecture compared to the RNN Model 1, but it's still developed as a solution to a classification problem.

* Compared to SimpleRNN for Model 1, this model consists of BidirectionalRNN

* We also applied one-hot encoding
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense, Dropout, Bidirectional

"""Load Dataset"""

df = pd.read_csv("/content/RMP_clean.csv")
df.info()

df.isna().sum()

"""Sentiment Function"""

# Define sentiment function
def sentiment(value):
    if int(value) <= 3:
        return 0
    else:
        return 1

# Apply sentiment function to student_difficult and student_star columns
df['student_difficult'] = df.student_difficult.apply(sentiment)
df['student_star'] = df.student_star.apply(sentiment)

df['student_difficult'].unique()
df['student_star'].unique()

"""Tokenize comments And Pad Sequences"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df.comments)
sequences = tokenizer.texts_to_sequences(df.comments)
word_index = tokenizer.word_index

MAX_SEQUENCE_LENGTH = 100
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

"""Encode Labels and Apply one-hot Encoding"""

le = LabelEncoder()
labels = le.fit_transform(df.student_difficult)
num_classes = len(le.classes_)

labels = pd.get_dummies(labels).values

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

"""Model Architecture"""

EMBEDDING_DIM = 100
RNN_UNITS = 64

model = Sequential()
model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
model.add(Bidirectional(SimpleRNN(RNN_UNITS, return_sequences=False)))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

NUM_EPOCHS = 10
BATCH_SIZE = 64

history = model.fit(X_train, y_train,
                    epochs=NUM_EPOCHS,
                    batch_size=BATCH_SIZE,
                    validation_data=(X_test, y_test))

"""Evaluate Model"""

score = model.evaluate(X_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""### LSTM Model 3 in PyTorch

#### Libraries
"""

import torch
import torchtext
import os

device = torch.device("cpu")

if torch.cuda.is_available():
   print("Training on GPU")
   device = torch.device("cuda:0")

"""#### Dataset"""

from google.colab import drive
from zipfile import ZipFile
drive.mount('/content/drive')

print(os.listdir(os.getcwd()))

path_to_extract_to = os.path.join(os.getcwd(), 'RateMyprofessor_CSV_DATA')
if not os.path.exists(path_to_extract_to):
  os.makedirs(path_to_extract_to)
file_name = "/content/drive/MyDrive/RateMyProfessor SampleData, Contact hejibo@usee.tech for the whole 5G dataset.zip"
  
# opening the zip file in READ mode
with ZipFile(file_name, 'r') as zip:
    # printing all the contents of the zip file
    zip.printdir()
  
    # extracting all the files
    print('Extracting all the files now...')
    zip.extractall(path_to_extract_to)
    print('Done!')

"""##### Read The CVS files"""

import glob
import pandas as pd
csv_files = glob.glob(path_to_extract_to +'/*.csv')
all_df = pd.read_csv(csv_files[0])
for filename_index in range(len(csv_files) - 1):
  df = pd.read_csv(csv_files[filename_index + 1])
  all_df = pd.concat([all_df, df], ignore_index=True)

"""##### Drop student_difficult from Dataframe"""

print(all_df.columns.tolist())
print(all_df['student_difficult'].unique())
# Drop rows with empty text based on student_difficult column 
all_df = all_df.dropna(subset=['student_difficult'])
all_df = all_df.fillna("")
print(all_df['student_difficult'].unique())
all_df = all_df.astype({"student_difficult": int})
print(all_df.student_difficult.dtype)
all_df = all_df.astype({"student_difficult": int})
all_df_without_student_difficult = all_df.drop(['student_difficult'], axis=1) 
print(all_df_without_student_difficult.columns.tolist())

train_columns = all_df_without_student_difficult.columns.tolist()

# create a single sentence composed of all columns value concatenated to train on.
all_df_without_student_difficult['train_sentence'] = all_df_without_student_difficult[train_columns[0]].apply(str) + ". "
all_df['train_sentence'] = all_df[train_columns[0]].apply(str) + ". "
for col in train_columns[1:]:
  print(col)
  all_df_without_student_difficult['train_sentence'] = all_df_without_student_difficult['train_sentence'] + all_df_without_student_difficult[col].apply(str) + ". "
all_df['train_sentence'] = all_df_without_student_difficult['train_sentence'] 
print(all_df_without_student_difficult['train_sentence'][0])
print(all_df_without_student_difficult.iloc[0])

"""##### Drop all columns except the student_difficult and train_sentence"""

# The dataframe for training with student_difficult as label and train_sentence

all_df = all_df.drop(['professor_name', 'school_name', 'department_name', 'local_name', 'state_name', 'year_since_first_review', 'star_rating', 'take_again', 'diff_index', 'tag_professor', 'num_student', 'post_date', 'name_onlines', 'student_star', 'attence', 'for_credits', 'would_take_agains', 'grades', 'stu_tags', 'help_useful', 'help_not_useful', 'comments']
, axis=1)
all_df['student_difficult'] = all_df['student_difficult'] - 1 
print(all_df.columns.tolist())
columns = all_df.columns.tolist()

"""##### Trimming the sentences


"""

from sklearn.model_selection import train_test_split

train_test_ratio = 0.10
train_valid_ratio = 0.80

first_n_words = 200

def trim_string(x):
  x = x.split(maxsplit= first_n_words)
  x = ''.join(x[:first_n_words])
  return x

"""##### Split The dataset into Train, Valid and Test"""

df_1 = all_df[all_df['student_difficult'] == 0]
df_2 = all_df[all_df['student_difficult'] == 1]
df_3 = all_df[all_df['student_difficult'] == 2]
df_4 = all_df[all_df['student_difficult'] == 3]
df_5 = all_df[all_df['student_difficult'] == 4]

# Train-test split

df_1_full_train, df_1_test = train_test_split(df_1, train_size = train_test_ratio, random_state = 1)
df_2_full_train, df_2_test = train_test_split(df_2, train_size = train_test_ratio, random_state = 1)
df_3_full_train, df_3_test = train_test_split(df_3, train_size = train_test_ratio, random_state = 1)
df_4_full_train, df_4_test = train_test_split(df_4, train_size = train_test_ratio, random_state = 1)
df_5_full_train, df_5_test = train_test_split(df_5, train_size = train_test_ratio, random_state = 1)


# Train-valid split
df_1_train, df_1_valid = train_test_split(df_1_full_train, train_size = train_valid_ratio, random_state = 1)
df_2_train, df_2_valid = train_test_split(df_2_full_train, train_size = train_valid_ratio, random_state = 1)
df_3_train, df_3_valid = train_test_split(df_3_full_train, train_size = train_valid_ratio, random_state = 1)
df_4_train, df_4_valid = train_test_split(df_4_full_train, train_size = train_valid_ratio, random_state = 1)
df_5_train, df_5_valid = train_test_split(df_5_full_train, train_size = train_valid_ratio, random_state = 1)


# Concatenate splits of different labels
df_train = pd.concat([df_1_train, df_2_train, df_3_train, df_4_train, df_5_train], ignore_index=True, sort=False)
df_valid = pd.concat([df_1_valid, df_2_valid, df_3_valid, df_4_valid, df_5_valid], ignore_index=True, sort=False)
df_test = pd.concat([df_1_test, df_2_test, df_3_test, df_4_test, df_5_test], ignore_index=True, sort=False)

# Write preprocessed data
destination_folder_train = os.path.join(os.getcwd(), 'Dataset', 'train')
destination_folder_valid = os.path.join(os.getcwd(), 'Dataset', 'valid')
destination_folder_test = os.path.join(os.getcwd(), 'Dataset', 'test')
destination_folder = os.path.join(os.getcwd(), 'Dataset')
if not os.path.exists(destination_folder):
  os.makedirs(destination_folder)
  os.makedirs(destination_folder_train)
  os.makedirs(destination_folder_valid)
  os.makedirs(destination_folder_test)

df_test.to_csv(destination_folder_train + '/train.csv', index=False)
df_train.to_csv(destination_folder_valid + '/valid.csv', index=False)
df_valid.to_csv(destination_folder_test + '/test.csv', index=False)

destination_folder_train = os.path.join(os.getcwd(), 'Dataset', 'train')
destination_folder_valid = os.path.join(os.getcwd(), 'Dataset', 'valid')
destination_folder_test = os.path.join(os.getcwd(), 'Dataset', 'test')
destination_folder = os.path.join(os.getcwd(), 'Dataset')

"""##### Alternatives to Fields, ...  from torchtext.data"""

import torchtext.transforms as T
from torch.hub import load_state_dict_from_url

padding_idx = 1
bos_idx = 0
eos_idx = 2
max_seq_len = 256
xlmr_vocab_path = r"https://download.pytorch.org/models/text/xlmr.vocab.pt"
xlmr_spm_model_path = r"https://download.pytorch.org/models/text/xlmr.sentencepiece.bpe.model"

text_transform = T.Sequential(
    T.SentencePieceTokenizer(xlmr_spm_model_path),
    T.VocabTransform(load_state_dict_from_url(xlmr_vocab_path)),
    T.Truncate(max_seq_len - 2),
    T.AddToken(token=bos_idx, begin=True),
    T.AddToken(token=eos_idx, begin=False),
)

def my_collate(batch):
    data = [item['text'] for item in batch]
    target = [item['label'] for item in batch]
    #target = torch.LongTensor(target)
    return {"label": target, "text": data}

import numpy as np
import torchdata.datapipes as dp
from torch.utils.data import DataLoader

def filter_for_data(filename):
    return  filename.endswith(".csv")

def row_processor(row):
    return {"label": int(row[0]), "text": text_transform(row[1:])}

def build_datapipes(root_dir=".", shuffle=True):
    datapipe = dp.iter.FileLister(root_dir)

    datapipe = datapipe.filter(filter_fn=filter_for_data)
    datapipe = datapipe.open_files()
  
    datapipe = datapipe.parse_csv(delimiter=",", skip_lines=1)
    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader
    
    datapipe = datapipe.shuffle()
    datapipe = datapipe.map(row_processor)
    # datapipe = datapipe.batch(batch_size=32)

    return datapipe


train_datapipe = build_datapipes(root_dir=str(destination_folder_train))
train_datapipe = DataLoader(dataset=train_datapipe, batch_size=32, collate_fn=my_collate, num_workers=2)
valid_datapipe = build_datapipes(root_dir=destination_folder_valid)
valid_datapipe = DataLoader(dataset=valid_datapipe, batch_size=32, collate_fn=my_collate, num_workers=2)
test_datapipe = build_datapipes(root_dir=destination_folder_test)
test_datapipe = DataLoader(dataset=test_datapipe, batch_size=32, collate_fn=my_collate, num_workers=2)

import matplotlib.pyplot as plt
import torchtext.functional as F
# Preliminaries

# from torchtext.legacy.data import Field, TabularDataset, BucketIterator
# from torchtext import Field, TabularDataset, BucketIterator, Iterator

# Models

import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# Training

import torch.optim as optim

# Evaluation

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

"""#### Model"""

class LSTM(nn.Module):

    def __init__(self, dimension=128):
        super(LSTM, self).__init__()

        self.embedding = nn.Embedding(len(load_state_dict_from_url(xlmr_vocab_path)), 300)
        self.dimension = dimension
        self.lstm = nn.LSTM(input_size=300,
                            hidden_size=dimension,
                            num_layers=3,
                            batch_first=True,
                            bidirectional=True)
        self.drop = nn.Dropout(p=0.5)

        self.fc = nn.Linear(2*dimension, 5)

    def forward(self, text, text_len):

        text_emb = self.embedding(text)

        packed_input = text_emb #pack_padded_sequence(text_emb, text_emb, batch_first=True, enforce_sorted=False)
        packed_output, _ = self.lstm(packed_input)
        output, _ = packed_output, _ #pad_packed_sequence(packed_output, batch_first=True)

        out_forward = output[range(len(output)), text_len - 1, :self.dimension]
        out_reverse = output[:, 0, self.dimension:]
        out_reduced = torch.cat((out_forward, out_reverse), 1)
        text_fea = self.drop(out_reduced)
        
        text_fea = self.fc(text_fea)
        #text_fea = torch.squeeze(text_fea, 1)
        #text_out = torch.sigmoid(text_fea)
        text_out = text_fea

        return text_out

# Save and Load Functions

def save_checkpoint(save_path, model, optimizer, valid_loss):

    if save_path == None:
        return
    
    state_dict = {'model_state_dict': model.state_dict(),
                  'optimizer_state_dict': optimizer.state_dict(),
                  'valid_loss': valid_loss}
    
    torch.save(state_dict, save_path)
    print(f'Model saved to ==> {save_path}')


def load_checkpoint(load_path, model, optimizer):

    if load_path==None:
        return
    
    state_dict = torch.load(load_path, map_location=device)
    print(f'Model loaded from <== {load_path}')
    
    model.load_state_dict(state_dict['model_state_dict'])
    optimizer.load_state_dict(state_dict['optimizer_state_dict'])
    
    return state_dict['valid_loss']

def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):

    if save_path == None:
        return
    
    state_dict = {'train_loss_list': train_loss_list,
                  'valid_loss_list': valid_loss_list,
                  'global_steps_list': global_steps_list}
    
    torch.save(state_dict, save_path)
    print(f'Model saved to ==> {save_path}')


def load_metrics(load_path):

    if load_path==None:
        return
    
    state_dict = torch.load(load_path, map_location=device)
    print(f'Model loaded from <== {load_path}')
    
    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']

"""##### Training"""

def train(model,
          optimizer,
          criterion = nn.CrossEntropyLoss(),
          train_loader = train_datapipe,
          valid_loader = valid_datapipe,
          num_epochs = 5,
          eval_every = 50,
          file_path = destination_folder,
          best_valid_loss = float("Inf")):
    
    # initialize running values
    running_loss = 0.0
    valid_running_loss = 0.0
    global_step = 0
    train_loss_list = []
    valid_loss_list = []
    global_steps_list = []

    # training loop
    model.train()
    for epoch in range(num_epochs):
        for batch in iter(train_loader):          
            labels = torch.tensor(batch["label"]).to(device)
          
            batch_new = []
            for x in batch["text"]:
              batch_new.append(x[0])
            
            titletext = F.to_tensor(batch_new, padding_value=padding_idx).to(device)
            titletext_len = len(titletext[0])
            
            output = model(titletext, titletext_len)
            
            labels = labels.type(torch.LongTensor)
            loss = criterion(output, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # update running values
            running_loss += loss.item()
            global_step += 1

            # evaluation step
            if global_step % eval_every == 0:
                model.eval()
                with torch.no_grad():                    
                  # validation loop
                  for batch in iter(valid_loader):
                      labels = torch.tensor(batch["label"]).to(device)
          
                      batch_new = []
                      for x in batch["text"]:
                        batch_new.append(x[0])
            
                      titletext = F.to_tensor(batch_new, padding_value=padding_idx).to(device)
                      titletext_len = len(titletext[0])
                      # labels = labels.to(device)
                      # titletext = titletext.to(device)
                      # titletext_len = titletext_len.to(device)
                      output = model(titletext, titletext_len)
                      labels = labels.type(torch.LongTensor)
                      loss = criterion(output, labels)
                      valid_running_loss += loss.item()

                # evaluation
                average_train_loss = running_loss / eval_every
                average_valid_loss = valid_running_loss / 45 #len(iter(valid_loader))
                train_loss_list.append(average_train_loss)
                valid_loss_list.append(average_valid_loss)
                global_steps_list.append(global_step)

                # resetting running values
                running_loss = 0.0                
                valid_running_loss = 0.0
                model.train()

                # print progress
                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'
                      .format(epoch+1, num_epochs, global_step, num_epochs*250,
                              average_train_loss, average_valid_loss))
                
                # checkpoint
                if best_valid_loss > average_valid_loss:
                    best_valid_loss = average_valid_loss
                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)
                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    
    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    print('Finished Training!')


model = LSTM().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
train(model=model, optimizer=optimizer, num_epochs=4)

"""##### Train and Valid Losses """

train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')
plt.plot(global_steps_list, train_loss_list, label='Train')
plt.plot(global_steps_list, valid_loss_list, label='Valid')
plt.xlabel('Global Steps')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""##### Evaluation


"""

def evaluate(model, test_loader, version='title', threshold=0.5):
    y_pred = []
    y_true = []

    model.eval()
    with torch.no_grad():
        for batch in iter(test_loader):           
            
            labels = torch.tensor(batch["label"]).to(device)
          
            batch_new = []
            for x in batch["text"]:
              batch_new.append(x[0])
            
            titletext = F.to_tensor(batch_new, padding_value=padding_idx).to(device)
            titletext_len = len(titletext[0])
            #labels = labels.to(device)
            #titletext = titletext.to(device)
            #titletext_len = titletext_len.to(device)
            output = model(titletext, titletext_len)
            output = output.argmax(dim=-1)
            # print(output.argmax(dim=-1))
            # output = (output > threshold).int()
            y_pred.extend(output.tolist())
            y_true.extend(labels.tolist())
    
    print('Classification Report:')
    print(classification_report(y_true, y_pred, labels=[4,3,2,1,0], digits=4))
    
    cm = confusion_matrix(y_true, y_pred, labels=[4,3,2,1,0])
    ax= plt.subplot()
    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt="d")

    ax.set_title('Confusion Matrix')

    ax.set_xlabel('Predicted Labels')
    ax.set_ylabel('True Labels')

    ax.xaxis.set_ticklabels(['5', '4','3', '2', '1' ])
    ax.yaxis.set_ticklabels(['5', '4','3', '2', '1'])
    
    
best_model = LSTM().to(device)
optimizer = optim.Adam(best_model.parameters(), lr=0.001)

load_checkpoint(destination_folder + '/model.pt', best_model, optimizer)
evaluate(best_model, test_datapipe)

"""###Regression in Keras with Keras Embeddings and GloVe

####Importing all the libraries
"""

import pandas as pd
import numpy as np
import json
pd.set_option('display.max_columns', None)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer, one_hot
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model
from keras.optimizers import Adam

"""####Reshaping the data"""

df_reg = pd.read_csv("/content/drive/MyDrive/CPSC 585 Project 2 datasets/RMP_clean.csv")

df_reg.info()

df_reg

"""Here we are merging the student_star and student_difficult columns in y value."""

y = []
y_cols = df_reg.loc[:,['student_star','student_difficult']]

y_cols

y_np = y_cols.values

y = y_np.tolist()

y

print(type(y))

y = np.array(y)

print(type(y))

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_reg['comments'])

word_index = tokenizer.word_index
print("Unique Words:", len(word_index))

sequences = tokenizer.texts_to_sequences(df_reg['comments'])

sequences

X = pad_sequences(sequences ,padding='post', maxlen=100)

X

y.shape

X.shape

timesteps = 100
samples = X.shape[0]
X_reshape = X[:samples*timesteps].reshape(samples, timesteps, -1)

print(X_reshape.shape)

X_reshape

vocab_size = len(tokenizer.word_index)+1

"""#### Splitting the data"""

X_train, X_test, y_train, y_test = train_test_split(X_reshape, y, test_size=0.2, random_state=42)

X_train.shape

"""####Callbacks

#####Early Stopping
"""

es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1)

"""#####Model Checkpoint"""

filepath="/content/drive/MyDrive/best models/best_weights.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')

"""##### List of all callbacks"""

callbacks_list = [checkpoint]

"""#### Model 1 with Keras embeddings"""

batch_size = 64
epochs = 10
embed_vector_size = 100

model = Sequential()
model.add(Embedding(vocab_size, embed_vector_size, input_length=100, name="Embedding"))
model.add(LSTM(100, input_shape=(batch_size, X_reshape[1], X_reshape.shape[2]), return_sequences=True))
model.add(LSTM(50))
model.add(Dense(50, activation='relu'))
model.add(Dense(2, activation='relu'))

model.compile(loss="mse", optimizer='adam', metrics=['mse', 'mae'])

model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)

loss, mse, mae = model.evaluate(X_test, y_test)

"""#### Model 2 with Keras Embeddings, Flatten, dropout"""

batch_size_2 = 128
epochs_2 = 25
embed_vector_size_2 = 100

model_2 = Sequential()
model_2.add(Embedding(vocab_size, embed_vector_size_2, input_length=100, name="Embedding"))
model_2.add(LSTM(256, input_shape=(batch_size_2, X_reshape[1], X_reshape.shape[2]), return_sequences=True))
model_2.add(Dropout(0.3))
model_2.add(LSTM(128))
model_2.add(Flatten())
model_2.add(Dense(128, activation='linear'))
model_2.add(Dense(64, activation='linear'))
model_2.add(Dense(2, activation='linear'))

model_2.compile(loss="mse", optimizer='adam', metrics=['mae'])

model_2.summary()

history = model_2.fit(X_train, y_train, epochs=epochs_2, batch_size=batch_size_2, validation_data=[X_train, y_train], validation_split=0.25, callbacks=callbacks_list)

model_2 = load_model("/content/drive/MyDrive/best models/best_weights_1.hdf5")
loss, mae = model_2.evaluate(X_test, y_test)

"""#### Model 3 with more epochs and regularization

"""

batch_size_3 = 128
epochs_3 = 50
embed_vector_size_3 = 100
callbacks_model3 = [es, checkpoint]

model_3 = Sequential()
model_3.add(Embedding(vocab_size, embed_vector_size_3, input_length=100, name="Embedding"))
model_3.add(LSTM(256, input_shape=(batch_size_3, X_reshape[1], X_reshape.shape[2]), return_sequences=True))
model_3.add(Dropout(0.3))
model_3.add(LSTM(128))
model_3.add(Flatten())
model_3.add(Dense(128, kernel_regularizer='l2', activation='linear'))
model_3.add(Dense(64, kernel_regularizer='l2', activation='linear'))
model_3.add(Dense(2, activation='linear'))

model_3.compile(loss="mse", optimizer='adam', metrics=['mae'])

model_3.summary()

history_3 = model_3.fit(X_train, y_train, 
                        epochs=epochs_3,
                        batch_size=batch_size_3, 
                        validation_data=[X_train, y_train], 
                        validation_split=0.25, 
                        callbacks=callbacks_model3)

loss, mae = model_3.evaluate(X_test, y_test)

"""#### GloVe Embeddings"""

import os 
print(os.listdir("/content/drive/MyDrive/glove embeddings"))

GLOVE_DIR = "/content/drive/MyDrive/glove embeddings"

embeddings_index = {}
f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))

for line in f:
     values = line.split() #splitting the values and putting them into a list
     word = values[0] #taking the words only
     coefs = np.asarray(values[1:], dtype='float32') #taking the embeddings only
     embeddings_index[word] = coefs #putting them in the dictionary
f.close()

#Total word vectors
print('Total word vectors are %s ' % len(embeddings_index))

EMBEDDING_DIM = 100
embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))

for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

embedding_matrix

"""####Model 4 with GloVe Embeddings"""

MAX_SEQUENCE_LENGTH = 100

embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=False)

batch_size_4 = 128
epochs_4 = 50
embed_vector_size_4 = 100
callbacks_model4 = [checkpoint]

model_4 = Sequential()
model_4.add(Embedding(vocab_size, embed_vector_size_4, weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=False))
model_4.add(LSTM(256, input_shape=(batch_size_4, X_reshape[1], X_reshape.shape[2]), return_sequences=True))
model_4.add(Dropout(0.3))
model_4.add(LSTM(128))
model_4.add(Flatten())
model_4.add(Dense(128, kernel_regularizer='l2', activation='linear'))
model_4.add(Dropout(0.3))
model_4.add(Dense(64, kernel_regularizer='l2', activation='linear'))
model_4.add(Dropout(0.3))
model_4.add(Dense(2, activation='linear'))

optimizer = Adam(learning_rate=0.01)
model_4.compile(loss="mse", optimizer=optimizer, metrics=['mae'])
model_4.summary()

history_4 = model_4.fit(X_train, y_train, epochs=epochs_4, batch_size=batch_size_4, validation_data=[X_train, y_train], validation_split=0.25, callbacks=callbacks_model4)

"""####Testing GloVe Model

If you have a already trained model then you can use this to load it
"""

from keras.models import load_model
model_glove = load_model('/content/drive/MyDrive/best models/best_weights_glove_1.hdf5')

loss, mae = model_4.evaluate(X_test, y_test)

"""From the json dataset, we created a test set, which will be used as an input to the model."""

df_test = pd.read_csv("/content/drive/MyDrive/CPSC 585 Project 2 datasets/jsondata.csv")

df_test["Comment"].isna().sum()

df_test["Comment"].dropna()

df_test["Comment"].to_csv("jsoncomment.csv",index=False)

df_json = pd.read_csv("jsoncomment.csv")

df_json=df_json.astype(str)

df_json.isna().sum()

tokenizer_json = Tokenizer()
tokenizer_json.fit_on_texts(df_json["Comment"])

word_index_json = tokenizer_json.word_index
print("Unique Words:", len(word_index_json))

word_index_json

sequences_json = tokenizer_json.texts_to_sequences(df_json["Comment"])
X_json = pad_sequences(sequences_json ,padding='post', maxlen=100)

print(X_json)

prediction = model_4.predict(X_json)

# this prints the predicted target value
print('Predicted target value:', prediction)
print("Actual value:", y_test)

"""# Conclusion
This project allowed us to explore the different recurrent machine learning networks that are available and widely used. We were able to test different pretrained embeddings such as GloVe embeddings and other included ones in Keras. We explored different approaches to the problem outside the expected regression task such as a modified sentiment analysis approach where we determine whether a user will rate either "good" or "bad." We analyzed the data and found that with the given data, its easier to predict the star rating over the difficulty rating because the comments generally held more information on how much they like the class rather than the difficulty of it. 

Through this project, we were able to explore different interpretations of the task at hand and different ideas each of us had. Through the different models and ideas, we explored strictly regression, classification, and a mix of both using different architectures. This project helped us understand how there are many ways to approach a problem, and while there may be a "best" solution, there are also many other "great" solutions as well. For example, while the problem is best suited for regression, dividing up the labels into 5 classes can work decently as well given that the labels are integers between 1 and 5. Overall, this project taught us a lot about NLP and problem solving.
"""